# ML/AI Theory Foundations

## Essential Textbooks

### Machine Learning Theory
- **Probabilistic Machine Learning** (Murphy) — comprehensive modern treatment
- **Understanding Machine Learning** (Shalev-Shwartz & Ben-David) — theory-first approach
- **Foundations of Machine Learning** (Mohri, Rostamizadeh, Talwalkar) — rigorous foundations

### Optimization
- **Convex Optimization** (Boyd & Vandenberghe) — the bible, free online
- **Numerical Optimization** (Nocedal & Wright) — practical algorithms
- **First-Order Methods in Optimization** (Beck) — modern gradient methods

### Computational Complexity
- **Computational Complexity: A Modern Approach** (Arora & Barak) — definitive reference
- **The Nature of Computation** (Moore & Mertens) — accessible, broad

### Statistics & Probability
- **High-Dimensional Statistics** (Wainwright) — modern concentration, high-dim inference
- **Probability: Theory and Examples** (Durrett) — rigorous probability theory

### Deep Learning Theory
- **The Principles of Deep Learning Theory** (Roberts, Yaida, Hanin) — infinite-width theory

## Foundational Papers by Topic

### Generalization Theory
- **VC Dimension** — Vapnik & Chervonenkis (1971)
- **PAC Learning** — Valiant (1984)
- **Rademacher Complexity** — Bartlett & Mendelson (2002)
- **Understanding Deep Learning Requires Rethinking Generalization** — Zhang et al. (2017)
- **Benign Overfitting in Linear Regression** — Bartlett et al. (2020)

### Optimization Theory
- **SGD Convergence** — Robbins & Monro (1951)
- **Adam Optimizer** — Kingma & Ba (2015)
- **Sharp Minima Can Generalize** — Dinh et al. (2017)
- **Neural Networks and the Bias/Variance Decomposition** — Geman et al. (1992)

### Neural Network Theory
- **Neural Tangent Kernel** — Jacot et al. (2018)
- **Mean Field Theory of Batch Normalization** — Yang et al. (2019)
- **Lottery Ticket Hypothesis** — Frankle & Carlin (2019)
- **Scaling Laws for Neural Language Models** — Kaplan et al. (2020)

### Information Theory & Learning
- **Information Bottleneck** — Tishby et al. (2000)
- **Deep Learning and the Information Bottleneck** — Shwartz-Ziv & Tishby (2017)

### Approximation Theory
- **Universal Approximation** — Cybenko (1989), Hornik (1991)
- **Benefits of Depth** — Telgarsky (2016)
- **Neural Networks with Low Dimensional Bottleneck** — Suzuki (2018)

## Mathematical Prerequisites

### Core Math (must be fluent)
- Linear algebra (eigenvalues, SVD, matrix norms)
- Multivariable calculus (gradients, Hessians, Taylor expansion)
- Probability theory (concentration inequalities, martingales)
- Basic statistics (MLE, Bayesian inference, hypothesis testing)

### Theory-Specific Math
- Measure theory (for rigorous probability)
- Functional analysis (for kernel methods, infinite-width limits)
- Convex analysis (for optimization proofs)
- Combinatorics (for VC dimension, covering numbers)

## Reading Order Suggestion

1. **Start:** Murphy's Probabilistic ML (Chapters 1-10)
2. **Optimization:** Boyd's Convex Optimization (Chapters 1-5, 9-10)
3. **Theory:** Shalev-Shwartz (Chapters 1-7)
4. **Deep:** Roberts et al. Principles of Deep Learning Theory
5. **Complexity:** Arora & Barak (selected chapters based on interest)
